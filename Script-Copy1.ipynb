{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "984d4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\Users\\dharmendrkumar\\Downloads\\Data Engineering\\Youtube\\Data_Models.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "982a09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum, avg, col, count, to_date, count\n",
    "from pyspark.sql.functions import to_date, to_timestamp, unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5ed90481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"youtube\") \\\n",
    "    .config(\"spark.local.dir\", r\"C:\\Users\\dharmendrkumar\\Downloads\\Data Engineering\\Youtube\\temp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "05f1c460-bd2b-4167-8eba-1fdabdf7766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date and timestamp formats\n",
    "date_format = 'yy.dd.MM'\n",
    "timestamp_format = 'yyyy-MM-dd\\'T\\'HH:mm:ss.SSS\\'Z\\''\n",
    "\n",
    "# Read the CSV file with specified date and timestamp formats\n",
    "youtube_in_original = spark.read.csv(\n",
    "    r\"C:\\Users\\dharmendrkumar\\Downloads\\Data Engineering\\Youtube\\Data\\INvideos.csv\",\n",
    "    header=True,\n",
    "    schema=youtube_india_DM,\n",
    "    dateFormat=date_format,\n",
    "    timestampFormat=timestamp_format\n",
    ")\n",
    "\n",
    "# Convert 'trending_date' to DateType\n",
    "youtube_in_original = youtube_in_original.withColumn(\n",
    "    'trending_date',\n",
    "    to_date(unix_timestamp('trending_date', date_format).cast('timestamp'))\n",
    ")\n",
    "\n",
    "# Convert 'publish_time' to TimestampType\n",
    "youtube_in_original = youtube_in_original.withColumn(\n",
    "    'publish_time',\n",
    "    to_timestamp('publish_time', timestamp_format)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c681c1b",
   "metadata": {},
   "source": [
    "# Find ou the top 10 trending videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c0bd44d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+---------+-------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|       publish_time|                tags|    views|  likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+---------+-------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|FlsCjmMhFmw|   2017-12-12|YouTube Rewind: T...|   YouTube Spotlight|         24|2017-12-06 17:58:51|\"Rewind|\"\"Rewind ...|125432237|2912710| 1545017|       807558|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|YouTube Rewind 20...|\n",
      "|FlsCjmMhFmw|   2017-12-11|YouTube Rewind: T...|   YouTube Spotlight|         24|2017-12-06 17:58:51|\"Rewind|\"\"Rewind ...|113876217|2811216| 1470387|       787174|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|YouTube Rewind 20...|\n",
      "|FlsCjmMhFmw|   2017-12-10|YouTube Rewind: T...|   YouTube Spotlight|         24|2017-12-06 17:58:51|\"Rewind|\"\"Rewind ...|100911567|2656672| 1353650|       682890|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|YouTube Rewind 20...|\n",
      "|6ZfuNTqbHE8|   2017-12-07|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 89930713|2606663|   53011|       347982|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "|6ZfuNTqbHE8|   2017-12-06|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 87449453|2584674|   52176|       341571|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "|6ZfuNTqbHE8|   2017-12-05|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 84281319|2555411|   51008|       339708|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "|6ZfuNTqbHE8|   2017-12-04|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 80360459|2513102|   49170|       335920|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "|FlsCjmMhFmw|   2017-12-09|YouTube Rewind: T...|   YouTube Spotlight|         24|2017-12-06 17:58:51|\"Rewind|\"\"Rewind ...| 75969469|2251815| 1127805|       827755|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|YouTube Rewind 20...|\n",
      "|6ZfuNTqbHE8|   2017-12-03|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 74789251|2444956|   46172|       330710|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "|6ZfuNTqbHE8|   2017-12-02|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 66637636|2331352|   41154|       316185|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+---------+-------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# Replace NULL values in 'views' column with '0'\n",
    "youtube_in_original = youtube_in_original.na.fill({'views': 0})\n",
    "\n",
    "# Convert 'views' column to integer type\n",
    "youtube_in_original = youtube_in_original.withColumn('views', expr('CAST(views AS INT)'))\n",
    "\n",
    "# Sort in descending order based on 'views'\n",
    "youtube_in_sorted = youtube_in_original.sort(col(\"views\").desc())\n",
    "\n",
    "# Show the top 10 trending videos\n",
    "youtube_in_sorted.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "33a37e25-a0e9-483b-afe0-e33089e7e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: date (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = false)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "youtube_in_original.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9601a4c2-9cbc-4df5-9f06-1545407775e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_trending = youtube_in_sorted.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f98f01bf-5d79-4ab6-9286-d16182516f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+---------+-------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|       publish_time|                tags|    views|  likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+---------+-------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|FlsCjmMhFmw|   2017-12-12|YouTube Rewind: T...|   YouTube Spotlight|         24|2017-12-06 17:58:51|\"Rewind|\"\"Rewind ...|125432237|2912710| 1545017|       807558|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|YouTube Rewind 20...|\n",
      "|FlsCjmMhFmw|   2017-12-11|YouTube Rewind: T...|   YouTube Spotlight|         24|2017-12-06 17:58:51|\"Rewind|\"\"Rewind ...|113876217|2811216| 1470387|       787174|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|YouTube Rewind 20...|\n",
      "|FlsCjmMhFmw|   2017-12-10|YouTube Rewind: T...|   YouTube Spotlight|         24|2017-12-06 17:58:51|\"Rewind|\"\"Rewind ...|100911567|2656672| 1353650|       682890|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|YouTube Rewind 20...|\n",
      "|6ZfuNTqbHE8|   2017-12-07|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 89930713|2606663|   53011|       347982|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "|6ZfuNTqbHE8|   2017-12-06|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 87449453|2584674|   52176|       341571|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "|6ZfuNTqbHE8|   2017-12-05|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 84281319|2555411|   51008|       339708|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "|6ZfuNTqbHE8|   2017-12-04|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 80360459|2513102|   49170|       335920|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "|FlsCjmMhFmw|   2017-12-09|YouTube Rewind: T...|   YouTube Spotlight|         24|2017-12-06 17:58:51|\"Rewind|\"\"Rewind ...| 75969469|2251815| 1127805|       827755|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|YouTube Rewind 20...|\n",
      "|6ZfuNTqbHE8|   2017-12-03|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 74789251|2444956|   46172|       330710|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "|6ZfuNTqbHE8|   2017-12-02|Marvel Studios' A...|Marvel Entertainment|         24|2017-11-29 13:26:24|\"marvel|\"\"comics\"...| 66637636|2331352|   41154|       316185|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|There was an idea...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+---------+-------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "youtube_trending.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5afef251-e147-48d8-9c8c-0ab524f03f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "26f024ac-2558-45c1-b961-6e7751d6229d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1200.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 111.0 failed 1 times, most recent failure: Lost task 7.0 in stage 111.0 (TID 499) (USHYDDHARMENDR8.us.deloitte.com executor driver): java.io.FileNotFoundException: C:\\Users\\dharmendrkumar\\AppData\\Local\\Temp\\blockmgr-776b71ea-6ffc-4399-8fe5-c0388cb2c8b1\\14\\temp_shuffle_455b7eca-0436-4fe9-b218-3497e2f9b4e5 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:141)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:161)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:308)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\dharmendrkumar\\AppData\\Local\\Temp\\blockmgr-776b71ea-6ffc-4399-8fe5-c0388cb2c8b1\\14\\temp_shuffle_455b7eca-0436-4fe9-b218-3497e2f9b4e5 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:141)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:161)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:308)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[230], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43myoutube_in_sorted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mviews\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10000\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\dataframe.py:1234\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1200.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 111.0 failed 1 times, most recent failure: Lost task 7.0 in stage 111.0 (TID 499) (USHYDDHARMENDR8.us.deloitte.com executor driver): java.io.FileNotFoundException: C:\\Users\\dharmendrkumar\\AppData\\Local\\Temp\\blockmgr-776b71ea-6ffc-4399-8fe5-c0388cb2c8b1\\14\\temp_shuffle_455b7eca-0436-4fe9-b218-3497e2f9b4e5 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:141)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:161)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:308)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\dharmendrkumar\\AppData\\Local\\Temp\\blockmgr-776b71ea-6ffc-4399-8fe5-c0388cb2c8b1\\14\\temp_shuffle_455b7eca-0436-4fe9-b218-3497e2f9b4e5 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:141)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:161)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:308)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "youtube_in_sorted.where(col(\"views\") >= '10000').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0281e9b-05e9-45e6-b3ae-b0c5df0ca360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
